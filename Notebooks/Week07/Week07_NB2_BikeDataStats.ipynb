{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Inferential Statistics"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We'll use our transformed data now to illustrate some Pythonic inferential statistics calculations.\n* you will need to run the previous notebook in order to save the new csv file to import here"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "import pandas as pd\n\ndf = pd.read_csv('/home/jovyan/bike_transformed.csv')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Let's see whether the mean value of bike rentals ('cnt') is different for whether it is a holiday or not."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df.groupby('holiday')['cnt'].describe()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df.boxplot(column=['cnt'], by='holiday')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "sample_yes = df[(df['holiday'] == 1)]\nsample_no = df[(df['holiday'] == 0)]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "print(sample_yes.shape)\nprint(sample_no.shape)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Hm.... we could do this, but to do the t-test, we'd need the sample sizes to be the same, and only choosing 21 of 710 values seems somewhat dicey."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df.groupby('weathersit')['cnt'].describe()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This has three values, so we'd need to do multivariate analysis.... let's come back to this."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df.groupby('workingday')['cnt'].describe()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "That's kinda better.... 231 values for non-working-day."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df.boxplot(column=['cnt'], by='workingday')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "sample_yes = df[(df['workingday'] == 1)]\nsample_no = df[(df['workingday'] == 0)]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "print(sample_yes.shape)\nprint(sample_no.shape)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "sample_yes = sample_yes.sample(231, random_state=0)\n\n# for later: what effect would something like this have in contrast with a random sample?\n# sample_yes = sample_yes.sort_values(by='cnt')[0:231]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "print(sample_yes.shape)\nprint(sample_no.shape)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We then need to test that the samples have the same variance.  We can do this with Levene's test."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "from scipy import stats"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.levene(sample_yes['cnt'], sample_no['cnt'])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "p-value is above 5%, so we assume that we can accept the null hypothesis -- the variances are the same.\n* side-note:  this is not true for all the samples of 231 points from this data subset!\n* we are going to ignore that and move forward"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Another assumption for independent t-test: the distribution of the residuals between the two groups is a normal distribution."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "from sklearn.preprocessing import scale\nimport numpy as np\nimport matplotlib.pyplot as plt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "diff_res = scale(np.array(sample_yes['cnt']) - np.array(sample_no['cnt']))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "plt.hist(diff_res)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Looks normal, so that is good.\n\nWe can be even more specific about testing for a normal distribution by making the QQ plot.  This is available in the stats module as a 'probplot'."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.probplot(diff_res, plot=plt, dist='norm');"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Looks reasonably good.\n\nEven one more test to confirm:  the Shapiro-Wilks test for normality. (If test is not significant, then the sample is normally distributed)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.shapiro(diff_res)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "p-value is high, so sample is normally distributed.\n\n*Finally* we get to the t-test:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.ttest_ind(sample_yes['cnt'], sample_no['cnt'])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Lo and behold, the p-value is high so we have to accept the null hypothesis.  The means are not statistically different."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "But... as a final note, we must remember that we are comparing a subset of the yes's here."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "fig,ax = plt.subplots(1,2)\nsample_yes['cnt'].plot(kind='box',ax=ax[0])\nax[0].set_ylim([0,9000])\nsample_no['cnt'].plot(kind='box',ax=ax[1])\nax[1].set_ylim([0,9000])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Let's check for temperature:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df.groupby('temp')['cnt'].describe()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Let's look at hot vs cold days:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df['hot'] = df['temp'] > df['temp'].mean()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df.groupby('hot')['cnt'].describe()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df.boxplot(column=['cnt'], by='hot')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "sample_yes = df[(df['hot'] == 1)]\nsample_no = df[(df['hot'] == 0)]"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "print(sample_yes.shape)\nprint(sample_no.shape)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "sample_yes = sample_yes.sample(364, random_state=0)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "print(sample_yes.shape)\nprint(sample_no.shape)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We then need to test that the samples have the same variance.  We can do this with Levene's test."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.levene(sample_yes['cnt'], sample_no['cnt'])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "p-value is below 5%..... we must proceed cautiously."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Another assumption for independent t-test: the distribution of the residuals between the two groups is a normal distribution."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "diff_res = scale(np.array(sample_yes['cnt']) - np.array(sample_no['cnt']))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "plt.hist(diff_res)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Not quite normal...\n\nWe can be even more specific about testing for a normal distribution by making the QQ plot.  This is available in the stats module as a 'probplot'."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.probplot(diff_res, plot=plt, dist='norm');"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Looks reasonably good, but with a clearer deviation at higher quantile values.\n\nEven one more test to confirm:  the Shapiro-Wilks test for normality. (If test is not significant, then the sample is normally distributed)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.shapiro(diff_res)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "p-value is low, so again sample is not quite normally distributed.\n\nLet's still try the t-test:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.ttest_ind(sample_yes['cnt'], sample_no['cnt'])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Very low p-value, which would indicate a significant difference."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We can use another Python library [researchpy](https://researchpy.readthedocs.io/en/latest/ttest_documentation.html) to do a test for when the residuals aren't normally distributed: Welch's t-test."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "import researchpy as rp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "# note the equal_variances = False\ndescriptives, results = rp.ttest(sample_yes['cnt'], \n                                 sample_no['cnt'], \n                                 equal_variances=False)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "descriptives"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "print(results)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Here the p-value is low and we've done the proper t-test for when the two variances are not equal."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Let's go back to look at the weather.  The data documentation notes:\n* weathersit\n  1. Clear, Few clouds, Partly cloudy, Partly cloudy\n  2. Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n  3. Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n  4. Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df.groupby('weathersit')['cnt'].describe()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df.boxplot(column=['cnt'], by='weathersit')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Here we can use ANOVA, to cover all three at once."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.f_oneway(df.loc[df['weathersit'] == 1,['cnt']],\n               df.loc[df['weathersit'] == 2,['cnt']],\n               df.loc[df['weathersit'] == 3,['cnt']])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Teeny tiny p-value.  The means are definitely not the same between the three groups."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Another option (with a LOT more output info) is to use the statsmodels library.  See here for their [ANOVA](https://www.statsmodels.org/stable/examples/notebooks/generated/interactions_anova.html)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "from statsmodels.formula.api import ols"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "result = ols('cnt ~ C(weathersit)', data = df).fit()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "result.summary()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The above was for the One-Way ANOVA.  Let's explore the Two-Way ANOVA.\n\nWe need that, for example, when looking at both 'weathersit' and 'hot'."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "model = ols('cnt ~ C(hot)', df).fit()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "model.summary()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "model = ols('cnt ~ C(hot) + C(weathersit)', df).fit()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "model.summary()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "and we can use the statsmodels api to assess the anova of each category:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "import statsmodels.api as sm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "sm.stats.anova_lm(model)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "model = ols('cnt ~ C(hot) * C(weathersit)', df).fit()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "model.summary()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "sm.stats.anova_lm(model)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Finally, let's look at the paired t-test.\n\nHere we don't have paired data, but let's look at another dataset (from 'trangel' stats-with-python repo) with hypothetical before/after values of blood glucose readings for 40 people with diabetes."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df = pd.read_csv(\"https://raw.githubusercontent.com/trangel/stats-with-python/master/data/BG-db.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df[['BG 1','BG 2']].hist()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df[['BG 1','BG 2']].describe()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Relatively small difference in the mean.... is it significant?"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "df[['BG 1','BG 2']].boxplot()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Check assumptions:\n* samples are independent and random\n* distribution of the residuals should be normal\n* variances between the two groups are equal"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.levene(df['BG 1'], df['BG 2'])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Just above the 5%.  We assume the variances are equal."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "bg_diff = scale(np.array(df['BG 1']) - np.array(df['BG 2']))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "plt.hist(bg_diff)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.probplot(bg_diff, plot=plt, dist='norm');"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.shapiro(bg_diff)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "stats.ttest_rel(df['BG 1'], df['BG 2'])"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "To satisfy our qualms about normality, let's us researchpy and say that the variances are not equal and that we are using paired values.  This leads to the Wilcoxon signed-rank test."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": "rp.ttest(df['BG 1'], df['BG 2'], paired = True, equal_variances = False)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}